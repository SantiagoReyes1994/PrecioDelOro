# avance 7: Red neuronal 

primero cargamos la data, dado que esto es una red neuronal y por lo general para esto se usa tensorflow, se correran los siguientes chunks en códigos de python. Las redes neuronales son modelos muy poderosos para las predicción y reconocimiento de patrones auqneu generalmente suelen usarse en reconocimiento de imágenes, también pueden ser usadas en distintos contextos.

```{r}
library(reticulate)
py_install("pandas")
py_install("numpy")
py_install("tensorflow")
```



```{python}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
import matplotlib.pyplot as plt

# Cargar los datos desde el archivo CSV
data = pd.read_csv("Gold Price.csv")

# Convertir la columna Date a tipo datetime si aún no lo está
data['Date'] = pd.to_datetime(data['Date'])

# Convertir la columna Date a un formato numérico (por ejemplo, número de días desde una fecha de referencia)
data['NumericDate'] = (data['Date'] - data['Date'].min()) / np.timedelta64(1, 'D')

# Verificar las primeras filas de los datos para asegurarse de que la conversión haya sido exitosa
print(data.head())

```
Ahora procedemos a construir la red neuronal, vamos a dividir la data en entrenamiento y prueba como es el procedimiento estándar. Definimos price como la variable objetivo y las demás varaibles que queremos analizar las incluimos en X, cabe acalrar que una gran ventaja de este tipo de modelos es que se pueden incluir varias variables al análisis con mucha facilidad.

```{python}
# Dividir los datos en características (X) y variable objetivo (y)
X = data[['NumericDate', 'High', 'Low', 'Volume']].values
y = data['Price'].values

# Dividir los datos en entrenamiento y prueba (por ejemplo, 80% - 20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Escalar los datos (opcional, dependiendo del modelo utilizado)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Imprimir las dimensiones de los conjuntos de entrenamiento y prueba
print(f"Dimensiones de X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"Dimensiones de X_test: {X_test.shape}, y_test: {y_test.shape}")


```

Ahora definimos una ANN simple para hacer el análisis y la entrenamos. 

```{python}
# Definir el modelo secuencial de TensorFlow
model = Sequential([
    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(16, activation='relu'),
    Dense(1)  # Capa de salida para la predicción de Price
])

# Compilar el modelo
model.compile(optimizer='adam', loss='mean_squared_error')

# Entrenar el modelo
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

```
ahora procedemos a ver que tan preciso es el modelo y evaluarlo.

```{python}
# Evaluar el modelo con los datos de prueba
loss = model.evaluate(X_test, y_test)
print(f"Pérdida en los datos de prueba: {loss}")

# Predecir con el modelo entrenado
predictions = model.predict(X_test)

# Visualizar las predicciones versus los valores reales (opcional)
plt.figure(figsize=(10, 6))
plt.plot(data['Date'], data['Price'], label='Actual Price', marker='o')
plt.plot(data['Date'].iloc[len(X_train):], predictions, label='Predicted Price', marker='x')
plt.xlabel('Date')
plt.ylabel('Price')
plt.title('Actual vs Predicted Gold Price')
plt.legend()
plt.grid(True)
plt.show()

```
como se puede ver la precisión no es exactamente muy acertada aunque no mala, esto se debe a que la red que se implementa es más bien simple. Sin embargo, puede ser mejorada aplicando una CNN, que es mucho más fuerte. Veamos 



```{python}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
import matplotlib.pyplot as plt

# Cargar los datos desde el archivo CSV
data = pd.read_csv("Gold Price.csv")

# Convertir la columna Date a tipo datetime si aún no lo está
data['Date'] = pd.to_datetime(data['Date'])

# Convertir la columna Date a un formato numérico (por ejemplo, número de días desde una fecha de referencia)
data['NumericDate'] = (data['Date'] - data['Date'].min()) / np.timedelta64(1, 'D')

# Dividir los datos en características (X) y variable objetivo (y)
X = data[['NumericDate', 'High', 'Low', 'Volume']].values
y = data['Price'].values

# Dividir los datos en entrenamiento y prueba (por ejemplo, 80% - 20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Escalar los datos (opcional, dependiendo del modelo utilizado)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Reorganizar los datos para la entrada en CNN (número de muestras, número de pasos de tiempo, número de características)
X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Definir el modelo secuencial de TensorFlow con CNN más profunda
model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=64, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=32, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=32, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=16, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)  # Capa de salida para la predicción de Price
])

# Compilar el modelo
model.compile(optimizer='adam', loss='mean_squared_error')

# Resumen del modelo
model.summary()

# Entrenar el modelo
history = model.fit(X_train_cnn, y_train, epochs=100, batch_size=64, validation_data=(X_test_cnn, y_test))

# Evaluar el modelo con los datos de prueba
loss = model.evaluate(X_test_cnn, y_test)
print(f"Pérdida en los datos de prueba: {loss}")

# Predecir con el modelo entrenado
predictions = model.predict(X_test_cnn)



# Visualizar las predicciones versus los valores reales (opcional)
plt.figure(figsize=(10, 6))
plt.plot(data['Date'], data['Price'], label='Actual Price', marker='o')
plt.plot(data['Date'].iloc[len(X_train):], predictions, label='Predicted Price', marker='x')
plt.xlabel('Date')
plt.ylabel('Price')
plt.title('Actual vs Predicted Gold Price')
plt.legend()
plt.grid(True)
plt.show()

```

Como se puede apreciar al hacer la red más profunda y añadir una serie de activaciones inmediatamente s ehace más precisa, paso de una estimación cercana a una en varios casos quasi perfecta auqneu tiene una ligera tendencia a sobre estimar algunos valores. Ahora procedamos a realizar una predicción a diez años con esta CNN.


```{python}
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
import matplotlib.pyplot as plt

# Función para generar fechas futuras
def generate_future_dates(start_date, periods, freq='D'):
    future_dates = pd.date_range(start=start_date, periods=periods, freq=freq)
    return future_dates

# Cargar los datos desde el archivo CSV
data = pd.read_csv("Gold Price.csv")

# Convertir la columna Date a tipo datetime si aún no lo está
data['Date'] = pd.to_datetime(data['Date'])

# Convertir la columna Date a un formato numérico (por ejemplo, número de días desde una fecha de referencia)
data['NumericDate'] = (data['Date'] - data['Date'].min()) / np.timedelta64(1, 'D')

# Escalar los datos (opcional, dependiendo del modelo utilizado)
scaler = StandardScaler()
X = data[['NumericDate', 'High', 'Low', 'Volume']].values
X_scaled = scaler.fit_transform(X)

# Reorganizar los datos para la entrada en CNN (número de muestras, número de pasos de tiempo, número de características)
X_cnn = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))

# Definir el modelo secuencial de TensorFlow con CNN más profunda
model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_cnn.shape[1], 1)),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=64, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=32, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=32, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=16, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)  # Capa de salida para la predicción de Price
])

# Compilar el modelo
model.compile(optimizer='adam', loss='mean_squared_error')

# Entrenar el modelo con todos los datos disponibles
model.fit(X_cnn, data['Price'].values, epochs=100, batch_size=64)

# Generar fechas futuras para predecir
start_date = data['Date'].max() + pd.Timedelta(days=1)
future_dates = generate_future_dates(start_date, periods=3650)  # 3650 días = 10 años

# Convertir las fechas futuras a formato numérico y escalarlas
future_numeric_dates = (future_dates - data['Date'].min()) / np.timedelta64(1, 'D')
future_data = np.column_stack([future_numeric_dates, np.zeros_like(future_numeric_dates), np.zeros_like(future_numeric_dates), np.zeros_like(future_numeric_dates)])
future_data_scaled = scaler.transform(future_data)
X_future_cnn = future_data_scaled.reshape((future_data_scaled.shape[0], future_data_scaled.shape[1], 1))

# Realizar predicciones para las fechas futuras
future_predictions = model.predict(X_future_cnn)

# Crear un DataFrame con las fechas y las predicciones
future_df = pd.DataFrame({
    'Date': future_dates,
    'Predicted Price': future_predictions.flatten()
})

# Visualizar las predicciones
plt.figure(figsize=(12, 8))
plt.plot(data['Date'], data['Price'], label='Actual Price', marker='o')
plt.plot(future_df['Date'], future_df['Predicted Price'], label='Predicted Price', marker='x')
plt.xlabel('Date')
plt.ylabel('Price')
plt.title('Actual vs Predicted Gold Price')
plt.legend()
plt.grid(True)
plt.show()

```


El modelo actual muestra un buen desempeño al predecir los precios del oro utilizando los datos de prueba disponibles, lo cual indica que puede capturar eficazmente los patrones presentes en los datos históricos. Sin embargo, al enfrentarse a la tarea de realizar predicciones a largo plazo, particularmente en un escenario donde no hay datos de prueba adicionales, se evidencia una limitación significativa. Esto sugiere que la arquitectura actual de la red neuronal, aunque mejorada con capas convolucionales y totalmente conectadas para capturar relaciones complejas, puede no ser lo suficientemente robusta para modelar con precisión las variaciones futuras del precio del oro. La falta de datos futuros impide al modelo generalizar más allá de los patrones históricos aprendidos, subrayando la necesidad de una arquitectura aún más poderosa y una mejor selección de características para capturar factores externos y dinámicas del mercado que influencian los precios del oro a largo plazo.

Con lo que se concluye que a pesar de que una red neuronal es muy poderosa a la hora de predecir en base a conductas aprendidas en el corto plazo, pero la cantidad que trabajo que requeriría el construir una capaz de hacer predicciones  acertadas para el futuro no permite considerarlo una herramienta óptima para el caso, es por esto que generalmente se emplean para reconocimiento de imágenes ne vez de predicciones a largo plazo. 
